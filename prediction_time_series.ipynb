{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b417960",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd26649a",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3481befd",
   "metadata": {},
   "source": [
    "We first import the different libraries that we will be using for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41070f99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T12:32:00.283254Z",
     "start_time": "2024-11-22T12:32:00.265249Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae4ad70",
   "metadata": {},
   "source": [
    "We import our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3360016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"data/fires-time-series.xlsx\"\n",
    "try:\n",
    "    df=pd.read_excel(data_path)\n",
    "except Exception as error:\n",
    "    print(f\"Error while importing the excel file: {error}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3db8c4",
   "metadata": {},
   "source": [
    "### Group by weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f3111",
   "metadata": {},
   "source": [
    "Agrupamos los datos y obtenemos el número total de incendios y la superficie quemada por semana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56e11eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de que la columna 'fecha' sea datetime\n",
    "df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "\n",
    "# Agregar columna de semana (Año-Semana)\n",
    "df['semana'] = df['fecha'].dt.to_period('W').apply(lambda r: r.start_time)\n",
    "\n",
    "# Agrupar por semana\n",
    "df = df.groupby('semana').agg(\n",
    "    numero_incendios=('superficie', 'count'),  # Cuenta las filas\n",
    "    superficie_total=('superficie', 'sum')    # Suma la superficie\n",
    ").reset_index()\n",
    "\n",
    "# Crear rango completo de semanas desde el primer lunes de 1983 hasta la última fecha\n",
    "start_date = pd.Timestamp(\"1983-01-01\")\n",
    "end_date = df['semana'].max()\n",
    "\n",
    "# Generar rango de semanas completas\n",
    "full_weeks = pd.date_range(start=start_date, end=end_date, freq='W-MON')\n",
    "\n",
    "# Crear un DataFrame con las semanas completas\n",
    "weeks_df = pd.DataFrame({'semana': full_weeks})\n",
    "\n",
    "# Combinar con el DataFrame original (left join)\n",
    "df = weeks_df.merge(df, on='semana', how='left')\n",
    "\n",
    "# Rellenar valores NaN con 0 para las columnas numéricas\n",
    "df['numero_incendios'] = df['numero_incendios'].fillna(0).astype(int)\n",
    "df['superficie_total'] = df['superficie_total'].fillna(0)\n",
    "\n",
    "# Agregar columnas de año y número de semana\n",
    "df['año'] = df['semana'].dt.year\n",
    "df['semana_año'] = df['semana'].dt.isocalendar().week\n",
    "df['mes'] = df['semana'].dt.month\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e04996",
   "metadata": {},
   "source": [
    "### Split train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213593ee",
   "metadata": {},
   "source": [
    "Los datos para entrenamiento son el 80% inicial (1983-2011) y los de test el 20% restante (2012-2018)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb7131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en train y test\n",
    "df_train = df[df['año'] <= 2011].reset_index(drop=True) #Modificados\n",
    "df_test = df[df['año'] >= 2012].reset_index(drop=True)\n",
    "\n",
    "# Calcular tamaños de train y test\n",
    "train_size = len(df_train)\n",
    "test_size = len(df_test)\n",
    "total_size = len(df)\n",
    "\n",
    "# Calcular porcentajes\n",
    "train_percentage = (train_size / total_size) * 100\n",
    "test_percentage = (test_size / total_size) * 100\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"\\nTamaño total: {total_size}\")\n",
    "print(f\"Train: {train_size} filas ({train_percentage:.2f}%)\")\n",
    "print(f\"Test: {test_size} filas ({test_percentage:.2f}%)\")\n",
    "\n",
    "# Verificar los resultados\n",
    "print(\"Train:\")\n",
    "print(df_train.head(), \"\\n--------------\\n\", df_train.tail())\n",
    "print(\"\\nTest:\")\n",
    "print(df_test.head(), \"\\n--------------\\n\", df_test.tail()) # No se usará hasta el final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9362f0c",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65ac8e1",
   "metadata": {},
   "source": [
    "### Analyze train data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b5cca",
   "metadata": {},
   "source": [
    "Podemos realizar un breve análisis de los datos que usaremos para el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72aa6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO (Quitar seguramente o hacer manualmente con Data Wrangler)\n",
    "# Posibles cosas\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df_train, x=\"numero_incendios\", color=\"blue\", width=0.5)\n",
    "plt.title(\"Boxplot del Número de Incendios\")\n",
    "plt.xlabel(\"Número de Incendios\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df_train, x=\"superficie_total\", color=\"orange\", width=0.5)\n",
    "plt.title(\"Boxplot de Superficie Total\")\n",
    "plt.xlabel(\"Superficie Total (ha)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_train[\"semana\"], df_train[\"numero_incendios\"], marker=\"o\")\n",
    "plt.title(\"Número de Incendios por Semana\")\n",
    "plt.xlabel(\"Fecha\")\n",
    "plt.ylabel(\"Número de Incendios\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_train[\"semana\"], df_train[\"superficie_total\"], marker=\"o\", color=\"red\")\n",
    "plt.title(\"Superficie Total Afectada por Semana\")\n",
    "plt.xlabel(\"Fecha\")\n",
    "plt.ylabel(\"Superficie Total (ha)\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "df_monthly = df_train.groupby(\"mes\")[[\"numero_incendios\", \"superficie_total\"]].mean().reset_index()\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_monthly[\"numero_incendios\"].plot(kind=\"bar\", color=\"blue\", alpha=0.7, label=\"Número de Incendios\")\n",
    "df_monthly[\"superficie_total\"].plot(kind=\"line\", color=\"red\", marker=\"o\", label=\"Superficie Total (ha)\", secondary_y=True)\n",
    "plt.xticks(ticks=range(12), labels=[f'{i+1}' for i in range(12)], rotation=0)\n",
    "plt.title(\"Promedio Mensual de Incendios y Superficie Afectada\")\n",
    "plt.xlabel(\"Mes\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "corr = df_train[['numero_incendios', 'superficie_total']].corr(numeric_only=True)\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Mapa de Calor de Correlaciones\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a09937",
   "metadata": {},
   "source": [
    "### Time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b69e23",
   "metadata": {},
   "source": [
    "Tendremos que hacer 3 modelos con series temporales:\n",
    "- Predecir numero de incendios\n",
    "- Predecir hectareas quemadas\n",
    "- Predecir ambas cosas a la vez\n",
    "\n",
    "Después introducir también variables exógenas (meteorológicas)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde035b3",
   "metadata": {},
   "source": [
    "Definimos una función que nos permita transformar el dataframe haciendo 'windowing', indicando el número de semanas de la ventana, las variables predictoras y las variables exógenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ee1d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windowed_dataframe(df, n_weeks, target_vars, exogenous_vars):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame con windowing para múltiples variables objetivo y exógenas,\n",
    "    conservando la columna de semanas.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame con las columnas relevantes.\n",
    "        n_weeks (int): Número de semanas previas a incluir.\n",
    "        target_vars (list): Variables objetivo (a predecir).\n",
    "        exogenous_vars (list): Variables exógenas.\n",
    "        week_col (str): Nombre de la columna que contiene las semanas.\n",
    "    \n",
    "    Returns:\n",
    "        df_windowed (DataFrame): DataFrame con las variables de entrada, salida y columna de semanas.\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    features = target_vars + exogenous_vars\n",
    "    columns = [\"semana\"]  # Mantener la columna semana\n",
    "\n",
    "    # Crear nombres de columnas para las semanas previas\n",
    "    for week in range(-n_weeks, 0):\n",
    "        columns.extend([f\"{col}_week_{week}\" for col in features])\n",
    "    \n",
    "    # Agregar columnas para las variables objetivo actuales\n",
    "    columns.extend(target_vars)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for i in range(n_weeks, len(data)):\n",
    "        # Semana actual\n",
    "        current_week = data[\"semana\"].iloc[i]\n",
    "\n",
    "        # Inputs: Variables de las últimas n semanas\n",
    "        input_window = data[features].iloc[i-n_weeks:i].values.flatten().tolist()\n",
    "\n",
    "        # Outputs: Variables objetivo para la semana actual\n",
    "        output_window = data[target_vars].iloc[i].values.tolist()\n",
    "\n",
    "        # Combinar inputs, outputs y la semana actual\n",
    "        rows.append([current_week] + input_window + output_window)\n",
    "    \n",
    "    # Crear DataFrame final\n",
    "    df_windowed = pd.DataFrame(rows, columns=columns)\n",
    "    return df_windowed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02af98c",
   "metadata": {},
   "source": [
    "#### Modelo 1: Número de incendios\n",
    "Para el primer caso, utilizamos solo el número de incendios sin variables exógenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87cc0884",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vars = [\"numero_incendios\"]\n",
    "exogenous_vars = []\n",
    "\n",
    "# Supongamos que df_train ya contiene estas variables\n",
    "n_weeks = 10  # Número de semanas previas\n",
    "df_windowed_train = create_windowed_dataframe(df_train, n_weeks, target_vars, exogenous_vars)\n",
    "\n",
    "# Concatenar las últimas n semanas del train al inicio del test para poder predecir las primeras semanas de test\n",
    "df_test_extended = pd.concat([df_train.iloc[-n_weeks:], df_test]).reset_index(drop=True)\n",
    "df_windowed_test = create_windowed_dataframe(df_test_extended, n_weeks, target_vars, exogenous_vars)\n",
    "\n",
    "# Revisar los primeros registros\n",
    "df_windowed_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0030ed5",
   "metadata": {},
   "source": [
    "Vamos a probar a utilizar Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "773abe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Separamos en X e y los datos ya en ventana\n",
    "X_train = df_windowed_train.iloc[:, 1:-1]\n",
    "y_train = df_windowed_train.iloc[:, -1]\n",
    "X_test = df_windowed_test.iloc[:, 1:-1]\n",
    "y_test = df_windowed_test.iloc[:, -1]\n",
    "\n",
    "# Crear el modelo\n",
    "rf = RandomForestRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867bb111",
   "metadata": {},
   "source": [
    "Validación con TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22bf83be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Definir el modelo base\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Definir el parámetro de búsqueda\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Número de árboles en el bosque\n",
    "    'max_depth': [None, 10, 20, 30],  # Profundidad máxima del árbol\n",
    "    'min_samples_split': [2, 5, 10],  # Mínimo de muestras para dividir un nodo\n",
    "    'min_samples_leaf': [1, 2, 4],    # Mínimo de muestras en una hoja\n",
    "}\n",
    "\n",
    "# Configurar TimeSeriesSplit\n",
    "n_splits = 5  # Número de divisiones para validación\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Configurar GridSearchCV con TimeSeriesSplit\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=tscv, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Ajustar el GridSearchCV a los datos de entrenamiento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejor conjunto de hiperparámetros\n",
    "print(f\"Mejores parámetros: {grid_search.best_params_}\")\n",
    "print(f\"Mejor score : {-grid_search.best_score_}\")\n",
    "\n",
    "# Mejor modelo después de la búsqueda\n",
    "best_rf_1 = grid_search.best_estimator_\n",
    "\n",
    "# Predecir en el conjunto de test\n",
    "y_pred_test = best_rf_1.predict(X_test)\n",
    "\n",
    "# Evaluar en el conjunto de test final\n",
    "final_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "# Resultados\n",
    "print(f\"MAE en conjunto de test final: {final_mae:.4f}\")\n",
    "# print(f\"Predicciones en conjunto de test: {y_pred_test}\")\n",
    "\n",
    "plt.plot(y_test, label=\"Real\")\n",
    "plt.plot(y_pred_test, label=\"Predicción\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Semana\")\n",
    "plt.ylabel(\"Número de incendios\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f210b6",
   "metadata": {},
   "source": [
    "También probamos el entrenamiento con el modelo LSTM, el cual está pensando específicamente para series temporales, también permite múltiples variables de entrada y de salida, y exógenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15fad87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "\n",
    "# Semilla aleatoria para reproducir los resultados\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# Preparamos\n",
    "X_train_lstm = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "# print(\"X_train:\", X_train_lstm)\n",
    "\n",
    "X_test_lstm = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "# print(\"X_test:\", X_test_lstm)\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "errors = []\n",
    "\n",
    "for train_idx, val_idx in tscv.split(X_train_lstm):\n",
    "    X_train_fold, X_val_fold = X_train_lstm[train_idx], X_train_lstm[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    # Entrena el modelo en cada partición\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1], 1)),\n",
    "        LSTM(64, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=10, verbose=0, batch_size=32)\n",
    "\n",
    "    # Evalúa\n",
    "    y_val_pred = model.predict(X_val_fold)\n",
    "    errors.append(mean_absolute_error(y_val_fold, y_val_pred))\n",
    "\n",
    "# Promedia el error de validación\n",
    "print(\"Error promedio de validación cruzada:\", np.mean(errors))\n",
    "\n",
    "# Evaluar\n",
    "loss = model.evaluate(X_test_lstm, y_test.values)\n",
    "print(f\"MAE en el conjunto de test final: {loss}\")\n",
    "\n",
    "# Predicciones\n",
    "y_pred = model.predict(X_test_lstm)\n",
    "\n",
    "\n",
    "plt.plot(y_test.values, label=\"Real\")\n",
    "plt.plot(y_pred, label=\"Predicción\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Semana\")\n",
    "plt.ylabel(\"Número de incendios\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b508ea49",
   "metadata": {},
   "source": [
    "#### Modelo 2: Hectáreas Quemadas\n",
    "Para el segundo caso, utilizamos solo el número de hectáreas sin variables exógenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07b29aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vars = [\"superficie_total\"]\n",
    "exogenous_vars = []\n",
    "\n",
    "# Supongamos que df_train ya contiene estas variables\n",
    "n_weeks = 10  # Número de semanas previas\n",
    "df_windowed_train = create_windowed_dataframe(df_train, n_weeks, target_vars, exogenous_vars)\n",
    "\n",
    "# Concatenar las últimas n semanas del train al inicio del test para poder predecir las primeras semanas de test\n",
    "df_test_extended = pd.concat([df_train.iloc[-n_weeks:], df_test]).reset_index(drop=True)\n",
    "df_windowed_test = create_windowed_dataframe(df_test_extended, n_weeks, target_vars, exogenous_vars)\n",
    "\n",
    "# Revisar los primeros registros\n",
    "df_windowed_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a620d7",
   "metadata": {},
   "source": [
    "Vamos a probar a utilizar Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38cf6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Separamos en X e y los datos ya en ventana\n",
    "X_train = df_windowed_train.iloc[:, 1:-1]\n",
    "y_train = df_windowed_train.iloc[:, -1]\n",
    "X_test = df_windowed_test.iloc[:, 1:-1]\n",
    "y_test = df_windowed_test.iloc[:, -1]\n",
    "\n",
    "# Crear el modelo\n",
    "rf = RandomForestRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c794dc",
   "metadata": {},
   "source": [
    "Validación con TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5949c468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Definir el modelo base\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Definir el parámetro de búsqueda\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Número de árboles en el bosque\n",
    "    'max_depth': [None, 10, 20, 30],  # Profundidad máxima del árbol\n",
    "    'min_samples_split': [2, 5, 10],  # Mínimo de muestras para dividir un nodo\n",
    "    'min_samples_leaf': [1, 2, 4],    # Mínimo de muestras en una hoja\n",
    "}\n",
    "\n",
    "# Configurar TimeSeriesSplit\n",
    "n_splits = 5  # Número de divisiones para validación\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Configurar GridSearchCV con TimeSeriesSplit\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=tscv, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Ajustar el GridSearchCV a los datos de entrenamiento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejor conjunto de hiperparámetros\n",
    "print(f\"Mejores parámetros: {grid_search.best_params_}\")\n",
    "print(f\"Mejor score : {-grid_search.best_score_}\")\n",
    "\n",
    "# Mejor modelo después de la búsqueda\n",
    "best_rf_1 = grid_search.best_estimator_\n",
    "\n",
    "# Predecir en el conjunto de test\n",
    "y_pred_test = best_rf_1.predict(X_test)\n",
    "\n",
    "# Evaluar en el conjunto de test final\n",
    "final_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "# Resultados\n",
    "print(f\"MAE en conjunto de test final: {final_mae:.4f}\")\n",
    "# print(f\"Predicciones en conjunto de test: {y_pred_test}\")\n",
    "\n",
    "plt.plot(y_test, label=\"Real\")\n",
    "plt.plot(y_pred_test, label=\"Predicción\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Semana\")\n",
    "plt.ylabel(\"Hectáreas quemadas\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b640e33",
   "metadata": {},
   "source": [
    "También probamos el entrenamiento con el modelo LSTM, el cual está pensando específicamente para series temporales, también permite múltiples variables de entrada y de salida, y exógenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d797f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "\n",
    "# Semilla aleatoria para reproducir los resultados\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# Preparamos\n",
    "X_train_lstm = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "# print(\"X_train:\", X_train_lstm)\n",
    "\n",
    "X_test_lstm = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "# print(\"X_test:\", X_test_lstm)\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "errors = []\n",
    "\n",
    "for train_idx, val_idx in tscv.split(X_train_lstm):\n",
    "    X_train_fold, X_val_fold = X_train_lstm[train_idx], X_train_lstm[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    # Entrena el modelo en cada partición\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1], 1)),\n",
    "        LSTM(64, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=10, verbose=0, batch_size=32)\n",
    "\n",
    "    # Evalúa\n",
    "    y_val_pred = model.predict(X_val_fold)\n",
    "    errors.append(mean_absolute_error(y_val_fold, y_val_pred))\n",
    "\n",
    "# Promedia el error de validación\n",
    "print(\"Error promedio de validación cruzada:\", np.mean(errors))\n",
    "\n",
    "# Evaluar\n",
    "loss = model.evaluate(X_test_lstm, y_test.values)\n",
    "print(f\"MAE en el conjunto de test final: {loss}\")\n",
    "\n",
    "# Predicciones\n",
    "y_pred = model.predict(X_test_lstm)\n",
    "\n",
    "\n",
    "plt.plot(y_test.values, label=\"Real\")\n",
    "plt.plot(y_pred, label=\"Predicción\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Semana\")\n",
    "plt.ylabel(\"Hectáreas quemadas\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e436e1",
   "metadata": {},
   "source": [
    "#### Modelo 3: Número de incendios y hectáreas quemadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa61fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vars = [\"numero_incendios\", \"superficie_total\"]\n",
    "exogenous_vars = []\n",
    "\n",
    "# Supongamos que df_train ya contiene estas variables\n",
    "n_weeks = 5  # Número de semanas previas\n",
    "df_windowed_train = create_windowed_dataframe(df_train, n_weeks, target_vars, exogenous_vars)\n",
    "\n",
    "# Concatenar las últimas n semanas del train al inicio del test para poder predecir las primeras semanas de test\n",
    "df_test_extended = pd.concat([df_train.iloc[-n_weeks:], df_test]).reset_index(drop=True)\n",
    "df_windowed_test = create_windowed_dataframe(df_test_extended, n_weeks, target_vars, exogenous_vars)\n",
    "\n",
    "# Revisar los primeros registros\n",
    "df_windowed_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3fd28f",
   "metadata": {},
   "source": [
    "Vamos a probar a utilizar Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50ea1078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Separamos en X e y los datos ya en ventana\n",
    "X_train = df_windowed_train.iloc[:, 1:-2]\n",
    "y_train = df_windowed_train.iloc[:, -2:]\n",
    "print(y_train)\n",
    "X_test = df_windowed_test.iloc[:, 1:-2]\n",
    "y_test = df_windowed_test.iloc[:, -2:]\n",
    "\n",
    "# Crear el modelo\n",
    "rf = RandomForestRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a46a58",
   "metadata": {},
   "source": [
    "Validación con TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7487aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Define el modelo con hiperparámetros elegidos\n",
    "rf = MultiOutputRegressor(RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    random_state=42\n",
    "))\n",
    "\n",
    "# Configurar TimeSeriesSplit\n",
    "n_splits = 5  # Número de divisiones para validación\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Para almacenar los errores en cada split\n",
    "mae_scores = []\n",
    "\n",
    "# Realizar la validación cruzada manual\n",
    "for train_index, val_index in tscv.split(X_train):\n",
    "    # Dividir los datos en entrenamiento y validación utilizando índices posicionales\n",
    "    X_train_split, X_val_split = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_split, y_val_split = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    rf.fit(X_train_split, y_train_split)\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    y_pred = rf.predict(X_val_split)\n",
    "    \n",
    "    # Calcular MAE para ambas variables objetivo\n",
    "    mae_split = mean_absolute_error(y_val_split, y_pred, multioutput='raw_values')  # Devuelve un array con el MAE de cada salida\n",
    "    mae_scores.append(mae_split)\n",
    "\n",
    "# Calcular el promedio del MAE para cada variable objetivo\n",
    "mean_mae = np.mean(mae_scores, axis=0)\n",
    "\n",
    "print(f\"MAE promedio por variable objetivo: {mean_mae}\")\n",
    "print(f\"MAE combinado (media de todas las variables): {np.mean(mean_mae)}\")\n",
    "\n",
    "# Predecir en el conjunto de test\n",
    "y_pred_test = rf.predict(X_test)\n",
    "\n",
    "# Evaluar en el conjunto de test final\n",
    "final_mae1 = mean_absolute_error(y_test[\"numero_incendios\"], y_pred_test[:, 0])\n",
    "final_mae2 = mean_absolute_error(y_test[\"superficie_total\"], y_pred_test[:, 1])\n",
    "\n",
    "# Resultados\n",
    "print(f\"MAE en conjunto de test final (numero_incendios): {final_mae1:.4f}\")\n",
    "print(f\"MAE en conjunto de test final (superficie_total): {final_mae2:.4f}\")\n",
    "# print(f\"Predicciones en conjunto de test: {y_pred_test}\")\n",
    "\n",
    "plt.plot(y_test, label=\"Real\")\n",
    "plt.plot(y_pred_test, label=\"Predicción\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(y_test[\"numero_incendios\"].values, label=\"Real\")\n",
    "plt.plot(y_pred_test[:, 0], label=\"Predicción\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Semana\")\n",
    "plt.ylabel(\"Número de incendios\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(y_test[\"superficie_total\"].values, label=\"Real\")\n",
    "plt.plot(y_pred_test[:, 1], label=\"Predicción\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Semana\")\n",
    "plt.ylabel(\"Hectáreas quemadas\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874071f3",
   "metadata": {},
   "source": [
    "También probamos el entrenamiento con el modelo LSTM, el cual está pensando específicamente para series temporales, también permite múltiples variables de entrada y de salida, y exógenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b53504d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "\n",
    "# Semilla aleatoria para reproducir los resultados\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# Preparamos\n",
    "X_train_lstm = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "# print(\"X_train:\", X_train_lstm)\n",
    "\n",
    "X_test_lstm = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "# print(\"X_test:\", X_test_lstm)\n",
    "\n",
    "y_train_lstm = y_train.values.reshape((y_train.shape[0], y_train.shape[1], 1))\n",
    "# print(\"X_train:\", X_train_lstm)\n",
    "\n",
    "y_test_lstm = y_test.values.reshape((y_test.shape[0], y_test.shape[1], 1))\n",
    "# print(\"X_test:\", X_test_lstm)\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "errors = []\n",
    "\n",
    "for train_idx, val_idx in tscv.split(X_train_lstm):\n",
    "    X_train_fold, X_val_fold = X_train_lstm[train_idx], X_train_lstm[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train_lstm[train_idx], y_train_lstm[val_idx]\n",
    "\n",
    "    # Entrena el modelo en cada partición\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1], 1)),\n",
    "        LSTM(64, activation='relu'),\n",
    "        Dense(2) # Vamos a obtener 2 salidas\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=10, verbose=0, batch_size=32)\n",
    "\n",
    "    # Evalúa\n",
    "    y_val_pred = model.predict(X_val_fold)\n",
    "    errors.append(mean_absolute_error(y_val_fold, y_val_pred))\n",
    "\n",
    "# Promedia el error de validación\n",
    "print(\"Error promedio de validación cruzada:\", np.mean(errors))\n",
    "\n",
    "# Evaluar\n",
    "loss = model.evaluate(X_test_lstm, y_test.values)\n",
    "print(f\"MAE en el conjunto de test final: {loss}\")\n",
    "\n",
    "# Predicciones\n",
    "y_pred = model.predict(X_test_lstm)\n",
    "\n",
    "\n",
    "plt.plot(y_test.values, label=\"Real\")\n",
    "plt.plot(y_pred, label=\"Predicción\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(y_test[\"numero_incendios\"].values, label=\"Real\")\n",
    "plt.plot(y_pred[:, 0], label=\"Predicción\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Semana\")\n",
    "plt.ylabel(\"Número de incendios\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(y_test[\"superficie_total\"].values, label=\"Real\")\n",
    "plt.plot(y_pred[:, 1], label=\"Predicción\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Semana\")\n",
    "plt.ylabel(\"Hectáreas quemadas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c6a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(y_test[\"numero_incendios\"].values, label=\"Real\")\n",
    "# plt.plot(y_pred_test[:, 0], label=\"Predicción\")\n",
    "# plt.legend()\n",
    "# plt.xlabel(\"Semana\")\n",
    "# plt.ylabel(\"Número de incendios\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(y_test[\"superficie_total\"].values, label=\"Real\")\n",
    "# plt.plot(y_pred_test[:, 1], label=\"Predicción\")\n",
    "# plt.legend()\n",
    "# plt.xlabel(\"Semana\")\n",
    "# plt.ylabel(\"Hectáreas quemadas\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c9640",
   "metadata": {},
   "source": [
    "## Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0f488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6dcd49",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc88c4df",
   "metadata": {},
   "source": [
    "## Test Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0aa303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
