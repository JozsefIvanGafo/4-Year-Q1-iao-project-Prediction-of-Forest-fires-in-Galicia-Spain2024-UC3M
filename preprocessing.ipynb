{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b417960",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd26649a",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3481befd",
   "metadata": {},
   "source": [
    "We first import the different libraries that we will be using for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41070f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime \n",
    "from unidecode import unidecode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae4ad70",
   "metadata": {},
   "source": [
    "We import our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13d1dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"data/fires-all.csv\"\n",
    "try:\n",
    "    fires=pd.read_csv(data_path)\n",
    "except Exception as error:\n",
    "    print(f\"Error while importing the excel file: {error}\")\n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1719a509",
   "metadata": {},
   "source": [
    "We analyze the data and observe the type of data on each column and how many nulls values we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb79af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae4e65",
   "metadata": {},
   "source": [
    "### Remove unnecesary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d37d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop\n",
    "try:\n",
    "    fires.drop([\"id\",\"causa\",\"causa_supuesta\",\"causa_desc\",\n",
    "                \"muertos\",\"heridos\",\"time_ctrl\",\"time_ext\",\n",
    "                \"personal\",\"medios\",\"gastos\",\"perdidas\",\n",
    "                \"latlng_explicit\"],axis=1, inplace=True)\n",
    "except Exception as error:\n",
    "    print(f\"ERROR while droping the columns {error}\")\n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac7bfc",
   "metadata": {},
   "source": [
    "### Remove unnecesary rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322dc45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We only want the fires of the comunidad 3 (Galicia)\n",
    "fires=fires[fires[\"idcomunidad\"]==3]\n",
    "#We drop null values on lat and lng, beacsue we cannot find where the fire happend,\n",
    "#  and it is older data from  1968\n",
    "fires=fires.dropna(subset=[\"lat\",\"lng\"])\n",
    "#We remove the column idcomunidad as it is no longer need it\n",
    "fires.drop([\"idcomunidad\"],axis=1,inplace=True)\n",
    "#print how many null values are for each column\n",
    "fires.isna().sum(),\n",
    "#TODO: from the coordinates lat and lng obtain the blanks municipio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a9e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d571811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We save the data for the time series prediction\n",
    "data_path=\"data/fires-time-series.xlsx\"\n",
    "try:\n",
    "    fires.to_excel(data_path,index=False)\n",
    "except Exception as error:\n",
    "    print(f\"Error while exporting the data to the excel file: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3360016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"data/fires-time-series.xlsx\"\n",
    "try:\n",
    "    fires_time_series=pd.read_excel(data_path)\n",
    "except Exception as error:\n",
    "    print(f\"Error while importing the excel file: {error}\")\n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175fe2aa",
   "metadata": {},
   "source": [
    "### Añadir el tiempo para la prediccion con variables exogenas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd33b986",
   "metadata": {},
   "source": [
    "#### Filtramos las fechas que sean mas antiguas del 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c42990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solo tenemos informacion a  partir del 2005 de meteorologia\n",
    "#convertimos las fechas a tipo datetime (para una mejor compatibilidad)\n",
    "fires['fecha'] = pd.to_datetime(fires['fecha'], errors='coerce')\n",
    "#filtrar fechas que empiezen a partir del 2005 hasta el 2018\n",
    "fires = fires.loc[fires['fecha'] >= '2009-01-01']\n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valores_unicos = fires['idprovincia'].unique()\n",
    "print(valores_unicos)\n",
    "for valor in valores_unicos:\n",
    "    print((fires['idprovincia'] == valor).sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c03ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires[fires['idprovincia']==32].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd45e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Juntar por semanas en la provincia de Ourense (id32)\n",
    "# Convertir 'fecha' a datetime\n",
    "fires['fecha'] = pd.to_datetime(fires['fecha'])\n",
    "\n",
    "# Filtrar por la provincia 32\n",
    "fires = fires[fires['idprovincia'] == 32]\n",
    "\n",
    "fires.drop([\"lat\",\"lng\",\"idmunicipio\",\"municipio\",\"idprovincia\"],axis=1,inplace=True)\n",
    "\n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3cd30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir 'fecha' a formato datetime\n",
    "fires[\"fecha\"] = pd.to_datetime(fires[\"fecha\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "# Crear nuevas columnas para el año y la semana\n",
    "fires['Anno'] = fires['fecha'].dt.year\n",
    "fires['Semana'] = fires['fecha'].dt.isocalendar().week\n",
    "\n",
    "fires = fires.groupby([\"Anno\", \"Semana\"]).agg({\n",
    "    'superficie': 'sum',           # Suma de superficie\n",
    "    'fecha': 'count'               # Cuenta de filas agrupadas (número de incendios)\n",
    "}).rename(columns={'fecha': 'numero_incendios'}).round(2)\n",
    "\n",
    "fires= fires.reset_index()\n",
    "fires.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c2d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_header(header):\n",
    "    replacements = {\n",
    "        'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "        'ü': 'u', 'ñ': 'n', 'Á': 'A', 'É': 'E', 'Í': 'I',\n",
    "        'Ó': 'O', 'Ú': 'U', 'Ü': 'U', 'Ñ': 'N'\n",
    "    }\n",
    "    for spanish_char, english_char in replacements.items():\n",
    "        header = header.replace(spanish_char, english_char)\n",
    "    return header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72911a7f",
   "metadata": {},
   "source": [
    "#### Juntamos todos los datos en un unico csv por estacion meteo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006cccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos las estaciones de meteorologia\n",
    "stations=[\"C01_A Capela_\",\"C02_Boimorto_\",\"LU01_Castro de Rei_\",\"LU02_Monforte de Lemos_\"]\n",
    "# stations=[\"C01_A Capela_\"]\n",
    "station_data = {station: pd.DataFrame() for station in stations}\n",
    "\n",
    "dataset_all = pd.DataFrame()\n",
    "\n",
    "#Iteramos por todos los años que tenemos disponibles de datos meteorologicos\n",
    "for year in range(2009,2019):\n",
    "    #Actualizamos el data path\n",
    "    data_path=f\"data/{year}_{year+1}/\"\n",
    "    #Por cada estacion lo abrimos y lo juntamos\n",
    "    for station in stations:\n",
    "        try:\n",
    "            station_path=data_path+f\"{station}01_01_{year}_01_01_{year+1}.csv\"\n",
    "            aux = pd.read_csv(station_path, encoding=\"utf-16\")\n",
    "\n",
    "            aux.columns = [unidecode(col) for col in aux.columns]\n",
    "            aux = aux[[\"Fecha\", \"Temp Media (oC)\", \"Temp Max (oC)\", \"Temp Minima (oC)\" , \"Humedad Media (%)\", \"Humedad Max (%)\", \"Humedad Min (%)\", \"Velviento (m/s)\",\"DirViento (o)\", \"VelVientoMax (m/s)\", \"Radiacion (MJ/m2)\", \"Precipitacion (mm)\"]]\n",
    "\n",
    "            # dataset_all = pd.concat([dataset_all, aux], ignore_index=True)\n",
    "\n",
    "            #Guardamos en station data\n",
    "            station_data[station]=pd.concat([station_data[station],aux],ignore_index=True)\n",
    "        except pd.errors.ParserError as parse_error:\n",
    "            print(f\"[ERROR]: Parser error when reading {station_path}: {parse_error}\")\n",
    "        except FileNotFoundError as file_error:\n",
    "            print(f\"[ERROR]: File not found: {station_path}\")\n",
    "        except Exception as general_error:\n",
    "            print(f\"[ERROR]: General error occurred while reading the file: {general_error}\")\n",
    "    print(f\"Datos cargados para del año {year}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f9162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputacion \n",
    "\n",
    "aux = [\"Temp Media (oC)\", \"Temp Max (oC)\", \"Temp Minima (oC)\" , \"Humedad Media (%)\", \"Humedad Max (%)\", \"Humedad Min (%)\", \"Velviento (m/s)\",\"DirViento (o)\", \"VelVientoMax (m/s)\", \"Radiacion (MJ/m2)\", \"Precipitacion (mm)\"]\n",
    "#  Iteramos sobre cada DataFrame en el diccionario `station_data`\n",
    "for aux_value in aux :\n",
    "    for station, data in station_data.items():\n",
    "        try:\n",
    "            # Verificamos si el DataFrame no está vacío\n",
    "            if not data.empty:\n",
    "                \n",
    "                 # Reemplazamos valores vacíos (\"\" o None) con NaN\n",
    "                data.replace([\"\", None], pd.NA, inplace=True)\n",
    "                if aux_value in data.columns:\n",
    "                    # Calculamos la media ignorando los valores NaN\n",
    "                    column_mean = data[aux_value].mean(skipna=True).round(4)\n",
    "\n",
    "                    # Imputamos los valores vacíos (NaN) con la media calculada\n",
    "                    data[aux_value] = data[aux_value].fillna(column_mean)\n",
    "\n",
    "                    # Guardamos el DataFrame modificado de vuelta en el diccionario\n",
    "                    station_data[station] = data\n",
    "\n",
    "            else:\n",
    "                print(f\"El DataFrame de la estación {station} está vacío.\")\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"[ERROR]: Error al procesar la estación {station}: {error}\")\n",
    "\n",
    "print(\"Imputación completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84670977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteramos sobre cada estación y sus respectivos datos\n",
    "for station, data in station_data.items():\n",
    "    try:\n",
    "        # Convertimos la columna 'Fecha' a formato datetime\n",
    "        data[\"Fecha\"] = pd.to_datetime(data[\"Fecha\"], format=\"%d/%m/%Y\")\n",
    "                \n",
    "        # Creamos nuevas columnas para el año y la semana\n",
    "        data['Anno'] = data['Fecha'].dt.year\n",
    "        data['Semana'] = data['Fecha'].dt.isocalendar().week\n",
    "        \n",
    "        # Agrupamos por año y semana y calculamos la media y suvarianza\n",
    "        weekly_data = data.groupby(['Anno', 'Semana']).agg({\n",
    "            \"Temp Media (oC)\": ['mean', 'var'],\n",
    "            \"Temp Max (oC)\": ['mean', 'var'],\n",
    "            \"Temp Minima (oC)\": ['mean', 'var'],\n",
    "            \"Humedad Media (%)\": ['mean', 'var'],\n",
    "            \"Humedad Max (%)\": ['mean', 'var'],\n",
    "            \"Humedad Min (%)\": ['mean', 'var'],\n",
    "            \"Velviento (m/s)\": ['mean', 'var'],\n",
    "            \"DirViento (o)\": ['mean', 'var'],\n",
    "            \"VelVientoMax (m/s)\": ['mean', 'var'],\n",
    "            \"Precipitacion (mm)\": ['mean', 'var']\n",
    "        }).round(4)\n",
    "\n",
    "        # Aplanamos los nombres de columnas\n",
    "        weekly_data.columns = ['_'.join(col).strip() for col in weekly_data.columns.values]\n",
    "\n",
    "        # Reset index para tener un DataFrame estándar\n",
    "        weekly_data = weekly_data.reset_index()\n",
    "        \n",
    "        for column in weekly_data.columns:\n",
    "            if column not in ['Anno', 'Semana']:\n",
    "                weekly_data[f'{column}_Semana_Pasada'] = weekly_data.groupby('Anno')[column].shift(1)\n",
    "\n",
    "        # Guardar datos semanales en el diccionario\n",
    "        station_data[station] = weekly_data\n",
    "\n",
    "        #station_data[station]['Valor_Anterior'] = station_data[station]['Temp Media (oC)_mean'].shift(1)\n",
    "\n",
    "        print(f\"Procesamiento semanal completo para {station}\")\n",
    "    \n",
    "    except Exception as error:\n",
    "        print(f\"[ERROR]: Error al procesar los datos semanales para {station}: {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b89df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ahora, para cada estación, concatenamos todos los DataFrames y los guardamos en un archivo CSV único\n",
    "# Iteramos sobre cada estación y sus respectivos datos en el diccionario station_data\n",
    "for station, data in station_data.items():\n",
    "    try:\n",
    "        # Definimos el nombre del archivo de salida\n",
    "        output_filename = f\"data/estaciones/{station}01_01_2009_01_01_2019.csv\"\n",
    "        \n",
    "        # Guardamos los datos concatenados en un archivo CSV\n",
    "        data.to_csv(output_filename, index=False, encoding=\"utf-16\")\n",
    "        print(f\"Archivo guardado para {station}: {output_filename}\")\n",
    "    \n",
    "    except Exception as error:\n",
    "        # Manejo de errores al guardar los datos\n",
    "        print(f\"[ERROR]: Error al guardar los datos para {station}: {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00deb331",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_fires = pd.merge(station_data['LU02_Monforte de Lemos_'], fires, on=['Anno', 'Semana'], how='outer')\n",
    "weather_fires['superficie'] = weather_fires['superficie'].fillna(0)\n",
    "weather_fires['numero_incendios'] = weather_fires['numero_incendios'].fillna(0)\n",
    "weather_fires = weather_fires[weather_fires['Anno'] < 2019].reset_index(drop=True)\n",
    "# Filtrando las filas que NO cumplen con los valores establecidos\n",
    "weather_fires = weather_fires[~((weather_fires['Anno'] == 2009) & (weather_fires['Semana'] == 1))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5232535",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_fires.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_fires.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcd36ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We save the data with the weather\n",
    "data_path=\"data/fires-weather.csv\"\n",
    "try:\n",
    "    weather_fires.to_csv(data_path,index=False)\n",
    "except Exception as error:\n",
    "    print(f\"Error while exporting the data to the excel file: {error}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
